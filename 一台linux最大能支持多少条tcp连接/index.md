# 一台linux最大能支持多少条TCP连接



<!--more-->

Linux服务器TCP连接数到底有多少？

在网络开发的世界里，一个基础但常常被误解的问题是：一台Linux服务器到底能支持多少个TCP连接？🤔 很多人第一反应是65535，因为“听说端口号有这么多，那长连接也应该有这么多吧？” 但真的是这样吗？

要搞清楚这个问题，关键在于理解TCP连接的两端——客户端和服务器端。🔍 任何一台服务器，实际上既是服务器端，也是客户端。例如，对于你的后端接口来说，用户是客户端，但当你请求Redis或MySQL时，你又变成了客户端。

如果你不把服务器端的角色和客户端的角色分开来理解，这个问题会一直困扰你。😵 因此，我们需要从客户端和服务端两个方面来探讨。

🌐 从客户端的角度看，一个Linux机器作为客户端时，理论上可以建立无数个连接，只要资源充足。

🔧 从服务端的角度看，问题就复杂多了。服务端需要维护一个连接状态，如果连接数过多，可能会耗尽系统资源，导致性能下降甚至崩溃。因此，服务端的连接数通常受到系统资源和管理策略的限制。

💡 总结来说，一台Linux服务器能支持多少TCP连接，并不是一个简单的数字问题，而是需要综合考虑客户端和服务端的角色、系统资源和管理策略。在实际开发中，合理管理和优化连接数是提高系统性能和可靠性的关键。


<br/><br/>

## 一台linux服务器最大能打开的文件数
### 限制参数
我们知道在Linux中一切皆文件,那么一台服务器最大能打开多少个文件呢?


Linux上能打开的最大文件数量受三个参数影响,分别是:
* ·fs.file-max(系统级别参数):该参数描述了整个系统可以打开的最大文件数量。但是root用户不会受该参数限制(比如:现在整个系统打开的文化牛描述符数量已达到fs.file-max,此时root用户仍然可以使用ps、kill等命令或打开其其他文件描述符)。
* soft nofile(进程级别参数):限制单个进程上可以打开白的最大文件数。只能在Linux上配置一次,不能针对不同用户配置不同的值。
* fs.nr_open(进程级别参数):限制单个进程上可以打开的最大文件数。可以针对不同用户配置不同的值。


### 调整服务器能打开的最大文件数示例
假设想让进程可以打开100万个文件描述符,这里用修改conf文件的方式给出一个建议。

如果日后工作里有类似的需求可以作为参考。

``` bash
vim/etc/sysctl.conf

fs.file-max=1100000//系统级别设置成110万,多留点buffer
fs.nr open=110000//进程级别也设置成110万,因为要保证比hard nofile大
```

使上面的配置生效sysctl -p
``` bash
vim/etc/security/limits.conf

//用户进程级别都设置成100万
soft nofile 1000000
hard nofile 1000000
```


## 一台服务器最大能支持多少连接
我们知道TCP连接,从根本上看其实就是client和serve端在内存中维护的一组
【socket内核对象】(这里也对应着TCP四元组:源IP、源端口口、目标IP、目标端口)
他们只要能够找到对方,那么就算是一条连接。那么一台服务器最大能建立多少条连接
呢?
* 由于TCP连接本质上可以理解为是client-server端的一对socket内核对象,那么从理论
上将应该是【<span style="color: #00ccffff;"> 2^32(ip数)*2^16(端口数) </span>】条连接(约等于两百多万亿)。
* 但是实际上由于受其他软硬件的影响,我们一台服务器不可能能建立这么多连接(主
要是受CPU和内存限制)

如果只以ESTABLISH状态的连接来算(这些连接只是建立,但是不收发数据也不处理相关
的业务逻辑)那么一台服务器最大能建立多少连接呢?以一台4GB内存的服务器为例!
* 这种情况下,那么能建立的连接数量主要取决于【<span style="color: #00ccffff;"> 内存的大小 </span>】(因为如果是)
ESTABLISH状态的空闲连接,不会消耗CPU(虽然有TCP保活包传输,但这个影响非
常小,可以忽略不计)。
* 我们知道一条ESTABLISH状态的连接大约消耗【<span style="color: #00ccffff;"> 3.3KBI内存 </span>】,那么通过计算得知一台
4GB内存的服务器,【<span style="color: #00ccffff;"> 可以建立100w+的TCP连接 </span>】(当然这里只是计算所有的连接
都只建立连接但不发送和处理数据的情况,如果真实场景中有数据往来和处理(数据
接收和发送都需要申请内存,数据处理便需要CPU),那便会消耗更高的内存以及占
用更多的CPU,并发不可能达到100w+)。

上面讨论的都是建立连接的理想情况,在现实中如果有频繁的数据收发和处理(比如:
压缩、加密等),那么一台服务器能支撑1000连接都算好的了,所以一台服务器能支撑多
少连接还要结合具体的场景去分析,不能光靠理论值去算。抛开业务逻辑单纯的谈并发没
有太大的实际意义。

服务器的开销大头往往并不是连接本身,而是每条连接上的数据收发以及请求业务逻辑
处理!!!


## 一台客户端机器最多能发起多少条连接
我们知道客户端每和服务端建立一个连接便会消耗掉client端一个端口。一台机器的端口
范围是【0~65535】,那么是不是说一台client机器最多和一台服务端机器建立65535个
连接呢(这65535个端口里还有很多保留端口,可用端口可能只有64000个左右)?
开始实验之前我们先来check下手头机器上的端口数量的配置
``` bash
$ sysctl -a | grep ip_local_port_range
net.ipv4.ip_local_port_range = 15000  65000
```
通过上述内核参数的输出看到内核开放了50000个端口可以供TCP连接使用。

当Linux作为客户端建立连接的时候,最大连接数量是受内核参数net.ipv4.ip_local_port_range限制 mip_local_port_range是可配置的,最大理论范围是0-65535

对于只有1个ip的客户端来说,受限于ip_local_port_range参数,也受限于65535。但Linux可以配置多个ip,有几个ip,最大理论值就翻几倍

多张网卡不是必须的。即使只有一张网卡,也可以配置多ip。
k8s就是这么干的,在k8s里,一台物理机上可以部署多个pod。
但每一个pod都会被分配一个独立的ip,所以完全不用担心物理机上部署了过多的pod而影响你用的pod里的TCP连接数量。
在ip给你的那一刻,你的pod就和其它应用隔离开了。

##结论
* TCP连接的客户端机:每一个ip可建立的TCP连接理论受限于ip_loca_port_range参数,也受限于65535。但可以通过配置多ip的方式来加大自己的建立连接的能力。
* TCP连接的服务器机:每一个监听的端口虽然理论值很大,但这个数字没有实际意义。最大并发数取决你的内存大小,每一条静止状态的TCP连接大约需要吃 3.3K 的内存。


